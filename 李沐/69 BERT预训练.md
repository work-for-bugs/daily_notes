# NLP里的迁移学习
使用预训练好的模型来抽取词、句子的特征，例如word2vec或语言模型   
不更新预训练好的模型   
需要构建新的网络来抓取新任务需要的信息：word2vec忽略了时序信息，语言模型只看了一个方向   

# BERT的动机
基于微调的NLP模型   
预训练的模型抽取了足够多的信息   
新的任务只需要增加一个简单的输出层   

# BERT的架构
是只有编码器的Transformer   

